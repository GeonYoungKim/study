# 스파크 2 프로그래밍 정리

- ## 3장 클러스터 구축 - cloudera manager(=cm)를 사용하여 구축
    1. cm을 사용하여 구축할때는 사전작업이 몇가지 필요하다(sudo command가 가능한 계정으로 진행해야한다.)
        
        - ssh 연결
            1. ssh-keygen command을 통해 키 생성
            2. 마스터 서버(cm을 설치하는 서버를 분산 클러스터의 마스터 서버로 사용하였음)의 public key를 각 슬레이브 서버의 authorized_keys에 추가
                - authorized_keys는 ~/.ssh 디렉토리에 위치하도록 함.
            3. 최초 ssh 연결시 yes or no를 물어보기 때문에 최초 접속 후 cm을 이용하여 설치 진행.

        - symbolic link(/ disk full을 방지하기 위해) -> destination dir은 777로 chmod하였다.

            0. 저의 경우 destination을 위해  ```~/../cdh```  dir를 만들었습니다.
            1. log 관련 link 걸기
                - /opt/cloudera
                - /dfs
                - /impala
                - /yarn
                - /var/log/ [spark, statestore, zookeeper, hive, catalogd, cloudera-manager-installer, cloudera-scm-agent, cloudera-scm-alertpublisher, cloudera-scm-eventserver, cloudera-scm-firehose, hadoop-hdfs, hadoop-mapreduce, hadoop-yarn ... 자신이 사용할 서비스]
                - /var/lib [cloudera-host-monitor, cloudera-scm-eventserver, cloudera-service-monitor, hadoop-yarn, zookeeper ... 자신이 사용할 서비스]
                - ``` 서비스가 추가된다면? mv -> rmdir -> add symbolic link```
                -
                ```shell
                sudo mkdir -p /home1/cdh/opt/cloudera
                sudo mkdir -p /home1/cdh/log/catalogd
                sudo mkdir -p /home1/cdh/log/cloudera-scm-eventserver
                sudo mkdir -p /home1/cdh/log/hadoop-mapreduce
                sudo mkdir -p /home1/cdh/log/hue
                sudo mkdir -p /home1/cdh/log/statestore
                sudo mkdir -p /home1/cdh/log/cloudera-manager-installer
                sudo mkdir -p /home1/cdh/log/cloudera-scm-firehose
                sudo mkdir -p /home1/cdh/log/hadoop-yarn
                sudo mkdir -p /home1/cdh/log/impalad
                sudo mkdir -p /home1/cdh/log/zookeeper
                sudo mkdir -p /home1/cdh/log/cloudera-scm-agent 
                sudo mkdir -p /home1/cdh/log/hadoop-hbase
                sudo mkdir -p /home1/cdh/log/hbase
                sudo mkdir -p /home1/cdh/log/oozie
                sudo mkdir -p /home1/cdh/log/cloudera-scm-alertpublisher
                sudo mkdir -p /home1/cdh/log/hadoop-hdfs
                sudo mkdir -p /home1/cdh/log/hive
                sudo mkdir -p /home1/cdh/log/spark
                sudo mkdir -p /home1/cdh/dfs/nn 
                sudo mkdir -p /home1/cdh/yarn/nm
                sudo mkdir -p /home1/cdh/cloudera-host-monitor 
                sudo mkdir -p /home1/cdh/cloudera-service-monitor 
                sudo mkdir -p /home1/cdh/lib/hadoop-yarn/yarn/nm-recovery 
                sudo mkdir -p /home1/cdh/lib/zookeeper
                sudo mkdir -p /home1/cdh/lib/oozie
                sudo mkdir -p /home1/cdh/lib/cloudera-scm-eventserver
                sudo mkdir -p /home1/cdh/tmp 
                sudo mkdir -p /home1/cdh/impala
                sudo mkdir -p /home1/cdh/impala/impalad
                sudo mkdir -p /home1/cdh/solr

                sudo ln -s /home1/cdh/opt/cloudera/ /opt/cloudera
                sudo ln -s /home1/cdh/log/cloudera-scm-agent /var/log/cloudera-scm-agent 
                sudo ln -s /home1/cdh/log/catalogd /var/log/catalogd
                sudo ln -s /home1/cdh/log/cloudera-manager-installer /var/log/cloudera-manager-installer
                sudo ln -s /home1/cdh/log/cloudera-scm-agent /var/log/cloudera-scm-agent
                sudo ln -s /home1/cdh/log/cloudera-scm-alertpublisher /var/log/cloudera-scm-alertpublisher
                sudo ln -s /home1/cdh/log/cloudera-scm-eventserver /var/log/cloudera-scm-eventserver
                sudo ln -s /home1/cdh/log/cloudera-scm-firehose /var/log/cloudera-scm-firehose
                sudo ln -s /home1/cdh/log/hadoop-hbase /var/log/hadoop-hbase
                sudo ln -s /home1/cdh/log/hadoop-hdfs /var/log/hadoop-hdfs
                sudo ln -s /home1/cdh/log/hadoop-mapreduce /var/log/hadoop-mapreduce
                sudo ln -s /home1/cdh/log/hadoop-yarn /var/log/hadoop-yarn
                sudo ln -s /home1/cdh/log/hbase /var/log/hbase
                sudo ln -s /home1/cdh/log/hive /var/log/hive
                sudo ln -s /home1/cdh/log/impalad /var/log/impalad
                sudo ln -s /home1/cdh/log/oozie /var/log/oozie
                sudo ln -s /home1/cdh/log/spark /var/log/spark
                sudo ln -s /home1/cdh/log/statestore /var/log/statestore
                sudo ln -s /home1/cdh/log/zookeeper /var/log/zookeeper
                sudo ln -s /home1/cdh/lib/hadoop-yarn/ /var/lib/hadoop-yarn
                sudo ln -s /home1/cdh/lib/zookeeper /var/lib/zookeeper
                sudo ln -s /home1/cdh/cloudera-host-monitor  /var/lib/cloudera-host-monitor
                sudo ln -s /home1/cdh/cloudera-service-monitor /var/lib/cloudera-service-monitor                                             
                sudo ln -s /home1/cdh/lib/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver
                sudo ln -s /home1/cdh/dfs/ /dfs
                sudo ln -s /home1/cdh/yarn/ /yarn
                sudo ln -s /home1/cdh/impala/ /impala
                sudo ln -s /home1/cdh/tmp /tmp

                sudo chmod 777 -R /home1/cdh

                ```

        - /etc/hosts 수정
            1. 마스터 서버의 /etc/hosts 파일에 슬레이브 서버의 ip, domain을 추가.<br>
            ```
            ##.###.###.###  dev-geon-cloudera002-###.###.##
            ##.###.###.###  dev-geon-cloudera003-###.###.##
            ##.###.###.###  dev-geon-cloudera004-###.###.##

            ```
    2. cloudera manager installer binary 다운로드 및 실행.
        - ```wget https://archive.cloudera.com/cm6/6.2.0/cloudera-manager-installer.bin```
        - ```chmod 755 cloudera-manager-installer.bin```
        - ```sudo ./cloudera-manager-installer.bin```
        - install 화면은 모두 yes를 하여 설치 진행(oracle jdk, database 등등을 설치하며 cm process를 띄운다) : 7180 port로 웹 UI에 접근 가능.

    3. 7180 포트로 UI에 들어가 설치 진행.
        - 호스트 textbox = 호스트(마스터 & 슬레이브 서버)의 ip 혹은 domain리스트를 line by line으로 넣음
        - ssh로 사용하여 설치.
            - username은 sudo가 가능한 계정명 기입
            - key file을 선택(마스터 서버의 private key 파일)
            - 설치 진행.
    4. 설치 후 각 서비스의 heapDumpFile dir(/tmp)을 변경 -> disk가 충분한 path ```heapDump 용도로 /tmp 사용하는게 아니면 건들이지 말자...```

    5. 설치 시 경험기
        - hdfs의 permission 문제 발생 시
            - cm을 통해 hdfs의 만들어지는 파일 소유아래와 같다.
                - 소유계정 = hdfs
                - 소유 그룹 = supergroup
            - 정석적으로는 사용하는 계정을 hdfs으로 변경 혹은 소유 그룹에 계정을 넣어야한다.
            - 저의 경우에는 찾는법이 귀찮아서(사실 할줄몰라서..ㅎ) dfs.permissions의 설정값을 off 해놓았다.(hdfs의 권한 검사를 할지 option)
            
        - 스파크의 경우에는 각 executor들의 가용 heap사이즈의 따라 error(실제 경험기입니다.)
            
            - 1GB의 경우 spark 쉘 자체가 실행되지 않음 참고(https://www.cloudera.com/documentation/enterprise/latest/topics/spark_troubleshooting.html)
                <br>
                저의 경우에 VM을 통해 클러스터를 구축하였고 cloudera에서도 작은 VM으로 만들시 발생할 수 있으며 무료 cm에서는 아래와 같은 옵션을 주어 스파크 쉘에 진입하라고 되어있었습니다.<br>
                ```shell
                pyspark --executor-memory=500M
                ```
                이것은 각 executor의 heap 메모리를 지정하는 옵션입니다. <br>
                default는 1G 로 설정되어있습니다.

            - 500M의 경우 스파크 쉘은 진입되나 액션연산이 call되고 stage를 실행할때 error 발생.<br>
                1. 저의 경우는 600M로 올려 해결
                2. 문제로는 spark 작업을 돌릴때 메모리가 부족한 이유
                3. 부족했었던 이유는 개인적으로 아래와 같은 이유라고 생각
                    - 다른 작업도 하고 싶어 hdfs, spark, yarn을 제외하고 hive, hue, impala, oozie, zookeeper를 설치 및 서비스 on.
                    - 파이썬으로 rdd를 사용하여 trans 하는 과정에서의 부하.
    
        - cloudera manager 6.0v -> cloudera manager 6.2v update
            - 6.0의 경우 spark 2.2.0 제공, 쉘 실행하는 코드를 봐보면 실제로 deprecated된 코드 존재.
            - 6.1부터 spark 2.4.0 제공, 실제로 spark home에서 최신 tar.gz을 받아 까보면 코드 동일 확인.


